{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "\n",
    "Apache Spark is a lightning-fast cluster computing technology, designed for fast computation. It is based on Hadoop MapReduce and it extends the MapReduce model to efficiently use it for more types of computations, which includes interactive queries and stream processing.\n",
    "\n",
    "Spark is designed to cover a wide range of workloads such as batch applications, iterative algorithms, interactive queries and streaming\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable, partitioned collection of elements that can be operated on in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs are resilient, which means that if a node performing an operation in spark is lost, the dataset can be reconstructed. This is because spark knows lineage of each RDD, which is sequence of steps to create the RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs are distributed, which means data in RDD is divided into one or more partitions and distributed as in-memory collection of objects across worker nodes in the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs are datasets that consists of records. A record can be collection of fields like a row in relational db.\n",
    "RDDs are created in such a way that each partiton contains a unique set of records that can be operated independently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs once created are immutable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create a RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext('local', 'test_app') \n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating an RDD using textFile.                                                                                    \n",
    "sc.textFile(filename, minPartitions=None, use_unicode=True)       \n",
    "\n",
    "minPartitions is number of partions to create, if not provided, default is one partition per block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventsRDD = spark.read.text(\"users.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventsRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an RDD from database into DataFrames which are special type of RDD with schema. Creating RDD from relational database table using functions from SparkSession object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "countryDF = spark.read.jdbc(url=\"jdbc:mysql://localhost:3306/population\", table=\"pops\", properties={\"user\":\"root\", \"password\":\"root@123\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryDF.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running SQL Queries against a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.registerDataFrameAsTable(countryDF, \"countries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = spark.sql(\"select * from countries x where x.population >= (select max(y.population) from countries y where y.continent=x.continent)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an RDD programmatically.  \n",
    "\n",
    "sc.parallelize(c, numSlices=None)\n",
    "\n",
    "c --> collection, numSlices --> Num. of partitions to be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ints = sc.parallelize([1,2,3,4,5,6,7,8,9])\n",
    "ints.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rangeRDD = sc.range(1, 20, 2, 2)\n",
    "rangeRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Persistence and Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD persistence is an optimization technique in which saves the result of RDD evaluation. Using this we save the intermediate result so that we can use it further if required. It reduces the computation overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make persisted RDD through cache() and persist() methods. When we use the cache() method we can store all the RDD in-memory. We can persist the RDD in memory and use it efficiently across parallel operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between cache() and persist() is that using cache() the default storage level is MEMORY_ONLY while using persist() we can use various storage levels - MEMORY_ONLY, MEMORY_AND_DISK, MEMORY_ONLY_SER, MEMORY_AND_DISK_SER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = sc.range(0, 10000, 1, 2)\n",
    "evens = nums.filter(lambda x: x%2)\n",
    "evens.persist()\n",
    "evens.count()\n",
    "evens.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic RDD Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 1, 4, 9, 16, 25]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD.map(<function>, preservePartitioning=[True, if True partitioning is preserved])\n",
    "rdd = sc.parallelize([1,2,3,4,1,2,3,4,5])\n",
    "rdd1 = rdd.map(lambda x: x**2)\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 16, 9, 16, 25]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.filter(lambda x: x > 4).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 4, 3, 9, 4, 16, 1, 1, 2, 4, 3, 9, 4, 16, 5, 25]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = rdd.flatMap(lambda x :[x, x*x])\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 9, 16, 25]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1,BarackObama,Barack Obama',\n",
       " u'2,ladygaga,Goddess of Love',\n",
       " u'3,jeresig,John Resig',\n",
       " u'4,justinbieber,Justin Bieber',\n",
       " u'6,matei_zaharia,Matei Zaharia',\n",
       " u'7,odersky,Martin Odersky',\n",
       " u'8,anonsys']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = sc.textFile(\"users.txt\")\n",
    "b.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'1', [u'1,BarackObama,Barack Obama']),\n",
       " (u'3', [u'3,jeresig,John Resig']),\n",
       " (u'2', [u'2,ladygaga,Goddess of Love']),\n",
       " (u'4', [u'4,justinbieber,Justin Bieber']),\n",
       " (u'7', [u'7,odersky,Martin Odersky']),\n",
       " (u'6', [u'6,matei_zaharia,Matei Zaharia']),\n",
       " (u'8', [u'8,anonsys'])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.groupBy(lambda x: x[0]).collect()\n",
    "b.groupBy(lambda x:x[0]).map(lambda x: (x[0], list(x[1]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'ALPHABET CITY,R4-CONDOMINIUM,25,2001,27798,792799,28.52,309114,11.12,483685,3212028,115.55,Manhattan',\n",
       " u'ALPHABET CITY,R4-CONDOMINIUM,23,2005,34734,1202838,34.63,414029,11.92,788809,5840002,168.14,Manhattan',\n",
       " u'ALPHABET CITY,R4-CONDOMINIUM,5,2008,7405,263988,35.65,60647,8.19,203341,1501999,202.84,Manhattan',\n",
       " u'ALPHABET CITY,R4-CONDOMINIUM,47,2005,36472,1454868,39.89,319859,8.77,1135009,8579000,235.22,Manhattan',\n",
       " u'ALPHABET CITY,R2-CONDOMINIUM,13,1920,18990,518047,27.28,206991,10.9,311056,2244001,118.17,Manhattan',\n",
       " u'ALPHABET CITY,R9-CONDOMINIUM,30,1901,20940,587576,28.06,264682,12.64,322894,2324000,110.98,Manhattan',\n",
       " u'ALPHABET CITY,R4-CONDOMINIUM,16,2000,15704,541631,34.49,210120,13.38,331511,2455000,156.33,Manhattan',\n",
       " u'ALPHABET CITY,R9-CONDOMINIUM,78,2001,65832,2518732,38.26,789984,12,1728748,13083000,198.73,Manhattan',\n",
       " u'ALPHABET CITY,R4-CONDOMINIUM,83,1928,78982,2138833,27.08,853006,10.8,1285827,9281001,117.51,Manhattan',\n",
       " u'ALPHABET CITY,R4-CONDOMINIUM,12,2005,11603,469341,40.45,149679,12.9,319662,2415000,208.14,Manhattan']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housingRDD = sc.textFile(\"housing.csv\")\n",
    "housingRDD.sortBy(lambda x: x[0]).take(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RDD Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. count() and collect() evaluates an RDD and all its parent and returns a value.\n",
    "2. saveAsTextFile() saves data externally\n",
    "3. foreach() is an action that performs a function on each element of an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme = sc.textFile(\"README.md\").flatMap(lambda x: x.split(' '))\n",
    "readme.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'#',\n",
       " u'Apache',\n",
       " u'Spark',\n",
       " u'',\n",
       " u'Spark',\n",
       " u'is',\n",
       " u'a',\n",
       " u'fast',\n",
       " u'and',\n",
       " u'general',\n",
       " u'cluster',\n",
       " u'computing',\n",
       " u'system',\n",
       " u'for',\n",
       " u'Big',\n",
       " u'Data.',\n",
       " u'It',\n",
       " u'provides',\n",
       " u'high-level',\n",
       " u'APIs',\n",
       " u'in',\n",
       " u'Scala,',\n",
       " u'Java,',\n",
       " u'Python,',\n",
       " u'and',\n",
       " u'R,',\n",
       " u'and',\n",
       " u'an',\n",
       " u'optimized',\n",
       " u'engine',\n",
       " u'that',\n",
       " u'supports',\n",
       " u'general',\n",
       " u'computation',\n",
       " u'graphs',\n",
       " u'for',\n",
       " u'data',\n",
       " u'analysis.',\n",
       " u'It',\n",
       " u'also',\n",
       " u'supports',\n",
       " u'a',\n",
       " u'rich',\n",
       " u'set',\n",
       " u'of',\n",
       " u'higher-level',\n",
       " u'tools',\n",
       " u'including',\n",
       " u'Spark',\n",
       " u'SQL',\n",
       " u'for',\n",
       " u'SQL',\n",
       " u'and',\n",
       " u'DataFrames,',\n",
       " u'MLlib',\n",
       " u'for',\n",
       " u'machine',\n",
       " u'learning,',\n",
       " u'GraphX',\n",
       " u'for',\n",
       " u'graph',\n",
       " u'processing,',\n",
       " u'and',\n",
       " u'Spark',\n",
       " u'Streaming',\n",
       " u'for',\n",
       " u'stream',\n",
       " u'processing.',\n",
       " u'',\n",
       " u'<http://spark.apache.org/>',\n",
       " u'',\n",
       " u'',\n",
       " u'##',\n",
       " u'Online',\n",
       " u'Documentation',\n",
       " u'',\n",
       " u'You',\n",
       " u'can',\n",
       " u'find',\n",
       " u'the',\n",
       " u'latest',\n",
       " u'Spark',\n",
       " u'documentation,',\n",
       " u'including',\n",
       " u'a',\n",
       " u'programming',\n",
       " u'guide,',\n",
       " u'on',\n",
       " u'the',\n",
       " u'[project',\n",
       " u'web',\n",
       " u'page](http://spark.apache.org/documentation.html)',\n",
       " u'and',\n",
       " u'[project',\n",
       " u'wiki](https://cwiki.apache.org/confluence/display/SPARK).',\n",
       " u'This',\n",
       " u'README',\n",
       " u'file',\n",
       " u'only',\n",
       " u'contains',\n",
       " u'basic',\n",
       " u'setup',\n",
       " u'instructions.',\n",
       " u'',\n",
       " u'##',\n",
       " u'Building',\n",
       " u'Spark',\n",
       " u'',\n",
       " u'Spark',\n",
       " u'is',\n",
       " u'built',\n",
       " u'using',\n",
       " u'[Apache',\n",
       " u'Maven](http://maven.apache.org/).',\n",
       " u'To',\n",
       " u'build',\n",
       " u'Spark',\n",
       " u'and',\n",
       " u'its',\n",
       " u'example',\n",
       " u'programs,',\n",
       " u'run:',\n",
       " u'',\n",
       " u'',\n",
       " u'',\n",
       " u'',\n",
       " u'',\n",
       " u'build/mvn',\n",
       " u'-DskipTests',\n",
       " u'clean',\n",
       " u'package',\n",
       " u'',\n",
       " u'(You',\n",
       " u'do',\n",
       " u'not',\n",
       " u'need',\n",
       " u'to',\n",
       " u'do',\n",
       " u'this',\n",
       " u'if',\n",
       " u'you',\n",
       " u'downloaded',\n",
       " u'a',\n",
       " u'pre-built',\n",
       " u'package.)',\n",
       " u'More',\n",
       " u'detailed',\n",
       " u'documentation',\n",
       " u'is',\n",
       " u'available',\n",
       " u'from',\n",
       " u'the',\n",
       " u'project',\n",
       " u'site,',\n",
       " u'at',\n",
       " u'[\"Building',\n",
       " u'Spark\"](http://spark.apache.org/docs/latest/building-spark.html).',\n",
       " u'',\n",
       " u'##',\n",
       " u'Interactive',\n",
       " u'Scala',\n",
       " u'Shell',\n",
       " u'',\n",
       " u'The',\n",
       " u'easiest',\n",
       " u'way',\n",
       " u'to',\n",
       " u'start',\n",
       " u'using',\n",
       " u'Spark',\n",
       " u'is',\n",
       " u'through',\n",
       " u'the',\n",
       " u'Scala',\n",
       " u'shell:',\n",
       " u'',\n",
       " u'',\n",
       " u'',\n",
       " u'',\n",
       " u'',\n",
       " u'./bin/spark-shell',\n",
       " u'',\n",
       " u'Try',\n",
       " u'the',\n",
       " u'following',\n",
       " u'command,',\n",
       " u'which',\n",
       " u'should',\n",
       " u'return',\n",
       " u'1000:',\n",
       " u'',\n",
       " u'',\n",
       " u'',\n",
       " u'',\n",
       " u'',\n",
       " u'scala>',\n",
       " u'sc.parallelize(1',\n",
       " u'to',\n",
       " u'1000).count()',\n",
       " u'',\n",
       " u'##',\n",
       " u'Interactive',\n",
       " u'Python',\n",
       " u'Shell',\n",
       " u'',\n",
       " u'Alternatively,',\n",
       " u'if',\n",
       " u'you',\n",
       " u'prefer',\n",
       " u'Python,',\n",
       " u'you',\n",
       " u'can',\n",
       " u'use',\n",
       " u'the',\n",
       " u'Python',\n",
       " u'shell:',\n",
       " u'',\n",
       " u'',\n",
       " u'',\n",
       " u'',\n",
       " u'',\n",
       " u'./bin/pyspark',\n",
       " u'',\n",
       " u'And',\n",
       " u'run',\n",
       " u'the',\n",
       " u'following',\n",
       " u'command,',\n",
       " u'which',\n",
       " u'should',\n",
       " u'also',\n",
       " u'return',\n",
       " u'1000:',\n",
       " u'',\n",
       " u'',\n",
       " u'',\n",
       " u'',\n",
       " u'',\n",
       " u'>>>',\n",
       " u'sc.parallelize(range(1000)).count()',\n",
       " u'',\n",
       " u'##',\n",
       " u'Example',\n",
       " u'Programs',\n",
       " u'',\n",
       " u'Spark',\n",
       " u'also',\n",
       " u'comes',\n",
       " u'with',\n",
       " u'several',\n",
       " u'sample',\n",
       " u'programs',\n",
       " u'in',\n",
       " u'the',\n",
       " u'`examples`',\n",
       " u'directory.',\n",
       " u'To',\n",
       " u'run',\n",
       " u'one',\n",
       " u'of',\n",
       " u'them,',\n",
       " u'use',\n",
       " u'`./bin/run-example',\n",
       " u'<class>',\n",
       " u'[params]`.',\n",
       " u'For',\n",
       " u'example:',\n",
       " u'',\n",
       " u'',\n",
       " u'',\n",
       " u'',\n",
       " u'',\n",
       " u'./bin/run-example',\n",
       " u'SparkPi',\n",
       " u'',\n",
       " u'will',\n",
       " u'run',\n",
       " u'the',\n",
       " u'Pi',\n",
       " u'example',\n",
       " u'locally.',\n",
       " u'',\n",
       " u'You',\n",
       " u'can',\n",
       " u'set',\n",
       " u'the',\n",
       " u'MASTER',\n",
       " u'environment',\n",
       " u'variable',\n",
       " u'when',\n",
       " u'running',\n",
       " u'examples',\n",
       " u'to',\n",
       " u'submit',\n",
       " u'examples',\n",
       " u'to',\n",
       " u'a',\n",
       " u'cluster.',\n",
       " u'This',\n",
       " u'can',\n",
       " u'be',\n",
       " u'a',\n",
       " u'mesos://',\n",
       " u'or',\n",
       " u'spark://',\n",
       " u'URL,',\n",
       " u'\"yarn\"',\n",
       " u'to',\n",
       " u'run',\n",
       " u'on',\n",
       " u'YARN,',\n",
       " u'and',\n",
       " u'\"local\"',\n",
       " u'to',\n",
       " u'run',\n",
       " u'locally',\n",
       " u'with',\n",
       " u'one',\n",
       " u'thread,',\n",
       " u'or',\n",
       " u'\"local[N]\"',\n",
       " u'to',\n",
       " u'run',\n",
       " u'locally',\n",
       " u'with',\n",
       " u'N',\n",
       " u'threads.',\n",
       " u'You',\n",
       " u'can',\n",
       " u'also',\n",
       " u'use',\n",
       " u'an',\n",
       " u'abbreviated',\n",
       " u'class',\n",
       " u'name',\n",
       " u'if',\n",
       " u'the',\n",
       " u'class',\n",
       " u'is',\n",
       " u'in',\n",
       " u'the',\n",
       " u'`examples`',\n",
       " u'package.',\n",
       " u'For',\n",
       " u'instance:',\n",
       " u'',\n",
       " u'',\n",
       " u'',\n",
       " u'',\n",
       " u'',\n",
       " u'MASTER=spark://host:7077',\n",
       " u'./bin/run-example',\n",
       " u'SparkPi',\n",
       " u'',\n",
       " u'Many',\n",
       " u'of',\n",
       " u'the',\n",
       " u'example',\n",
       " u'programs',\n",
       " u'print',\n",
       " u'usage',\n",
       " u'help',\n",
       " u'if',\n",
       " u'no',\n",
       " u'params',\n",
       " u'are',\n",
       " u'given.',\n",
       " u'',\n",
       " u'##',\n",
       " u'Running',\n",
       " u'Tests',\n",
       " u'',\n",
       " u'Testing',\n",
       " u'first',\n",
       " u'requires',\n",
       " u'[building',\n",
       " u'Spark](#building-spark).',\n",
       " u'Once',\n",
       " u'Spark',\n",
       " u'is',\n",
       " u'built,',\n",
       " u'tests',\n",
       " u'can',\n",
       " u'be',\n",
       " u'run',\n",
       " u'using:',\n",
       " u'',\n",
       " u'',\n",
       " u'',\n",
       " u'',\n",
       " u'',\n",
       " u'./dev/run-tests',\n",
       " u'',\n",
       " u'Please',\n",
       " u'see',\n",
       " u'the',\n",
       " u'guidance',\n",
       " u'on',\n",
       " u'how',\n",
       " u'to',\n",
       " u'[run',\n",
       " u'tests',\n",
       " u'for',\n",
       " u'a',\n",
       " u'module,',\n",
       " u'or',\n",
       " u'individual',\n",
       " u'tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).',\n",
       " u'',\n",
       " u'##',\n",
       " u'A',\n",
       " u'Note',\n",
       " u'About',\n",
       " u'Hadoop',\n",
       " u'Versions',\n",
       " u'',\n",
       " u'Spark',\n",
       " u'uses',\n",
       " u'the',\n",
       " u'Hadoop',\n",
       " u'core',\n",
       " u'library',\n",
       " u'to',\n",
       " u'talk',\n",
       " u'to',\n",
       " u'HDFS',\n",
       " u'and',\n",
       " u'other',\n",
       " u'Hadoop-supported',\n",
       " u'storage',\n",
       " u'systems.',\n",
       " u'Because',\n",
       " u'the',\n",
       " u'protocols',\n",
       " u'have',\n",
       " u'changed',\n",
       " u'in',\n",
       " u'different',\n",
       " u'versions',\n",
       " u'of',\n",
       " u'Hadoop,',\n",
       " u'you',\n",
       " u'must',\n",
       " u'build',\n",
       " u'Spark',\n",
       " u'against',\n",
       " u'the',\n",
       " u'same',\n",
       " u'version',\n",
       " u'that',\n",
       " u'your',\n",
       " u'cluster',\n",
       " u'runs.',\n",
       " u'',\n",
       " u'Please',\n",
       " u'refer',\n",
       " u'to',\n",
       " u'the',\n",
       " u'build',\n",
       " u'documentation',\n",
       " u'at',\n",
       " u'[\"Specifying',\n",
       " u'the',\n",
       " u'Hadoop',\n",
       " u'Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)',\n",
       " u'for',\n",
       " u'detailed',\n",
       " u'guidance',\n",
       " u'on',\n",
       " u'building',\n",
       " u'for',\n",
       " u'a',\n",
       " u'particular',\n",
       " u'distribution',\n",
       " u'of',\n",
       " u'Hadoop,',\n",
       " u'including',\n",
       " u'building',\n",
       " u'for',\n",
       " u'particular',\n",
       " u'Hive',\n",
       " u'and',\n",
       " u'Hive',\n",
       " u'Thriftserver',\n",
       " u'distributions.',\n",
       " u'See',\n",
       " u'also',\n",
       " u'[\"Third',\n",
       " u'Party',\n",
       " u'Hadoop',\n",
       " u'Distributions\"](http://spark.apache.org/docs/latest/hadoop-third-party-distributions.html)',\n",
       " u'for',\n",
       " u'guidance',\n",
       " u'on',\n",
       " u'building',\n",
       " u'a',\n",
       " u'Spark',\n",
       " u'application',\n",
       " u'that',\n",
       " u'works',\n",
       " u'with',\n",
       " u'a',\n",
       " u'particular',\n",
       " u'distribution.',\n",
       " u'',\n",
       " u'##',\n",
       " u'Configuration',\n",
       " u'',\n",
       " u'Please',\n",
       " u'refer',\n",
       " u'to',\n",
       " u'the',\n",
       " u'[Configuration',\n",
       " u'Guide](http://spark.apache.org/docs/latest/configuration.html)',\n",
       " u'in',\n",
       " u'the',\n",
       " u'online',\n",
       " u'documentation',\n",
       " u'for',\n",
       " u'an',\n",
       " u'overview',\n",
       " u'on',\n",
       " u'how',\n",
       " u'to',\n",
       " u'configure',\n",
       " u'Spark.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'#', u'Apache', u'Spark', u'', u'Spark']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.take(5) # returns first n element, but is unordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'your', u'you', u'works', u'with', u'will']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.distinct().top(5) # like take, but ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'#'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduce() is aggregate action, each of which performs a commutative or associative operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = sc.parallelize([1,2,3,4,5,6,7])\n",
    "numbers.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD.foreach(function)  applies a function to each element of RDD\n",
    "\n",
    "foreach(), can be used to apply a function to each element of RDD, which is not possible in case of map or flatMap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint(x):\n",
    "    print(x*x)\n",
    "\n",
    "l = []\n",
    "numbers = sc.parallelize([1,2,3,4,5])\n",
    "numbers.foreach(pprint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
